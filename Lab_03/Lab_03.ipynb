{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import hub\n",
    "import torchvision\n",
    "import torchsummary\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"params.yaml\", \"r\") as f:\n",
    "\tparams = yaml.safe_load(f)\n",
    "\n",
    "train_methods = params[\"train\"][f\"method_{params['method']}\"]\n",
    "del params[\"train\"]\n",
    "params[\"train\"] = train_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/francesco/.cache/torch/hub/pytorch_vision_main\n"
     ]
    }
   ],
   "source": [
    "model = hub.load(\"pytorch/vision\", \"resnet18\", weights=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      "  (1): Linear(in_features=1000, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "method = params[\"method\"]\n",
    "\n",
    "if method == 0:\n",
    "\tmodel.fc = torch.nn.Linear(512, 2).to(device)\n",
    "\n",
    "if method == 1:\n",
    "\tmodel.fc = torch.nn.Sequential(\n",
    "\t\ttorch.nn.Linear(512, 128),\n",
    "\t\ttorch.nn.ReLU(),\n",
    "\t\ttorch.nn.Linear(128, 2),\n",
    "\t).to(device)\n",
    "\n",
    "elif method == 2:\n",
    "\tmodel = torch.nn.Sequential(model, torch.nn.Linear(1000, 2).to(device))\n",
    "\n",
    "else:\n",
    "\traise ValueError(\"Method not recognised\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchsummary.summary(model, (3, 244, 244))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(folder, class_name, class_val):\n",
    "\treturn [\n",
    "\t\t(os.path.join(folder, class_name, path), class_val)\n",
    "\t\tfor path in os.listdir(os.path.join(folder, class_name))\n",
    "\t\tif path.endswith(\".jpg\")\n",
    "\t]\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, folder, transform=None):\n",
    "\t\tself.__transform = transform\n",
    "\t\t\n",
    "\t\tcats = get_images(folder, \"cats\", 0)\n",
    "\t\tdogs = get_images(folder, \"dogs\", 1)\n",
    "\t\t\n",
    "\t\tself.__data = [*cats, *dogs]\n",
    "\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.__data)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\timg = Image.open(self.__data[index][0])\n",
    "\t\treturn self.__transform(img), self.__data[index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "\t# torchvision.transforms.RandomRotation(15),\n",
    "\t# torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
    "])\n",
    "\n",
    "train_set = Dataset(\"./data/training_set/training_set\", transform)\n",
    "test_set = Dataset(\"./data/test_set/test_set\", transform)\n",
    "\n",
    "train_set, dev_set = torch.utils.data.random_split(train_set, [.9, .1])\n",
    "\n",
    "train_set_loader = torch.utils.data.DataLoader(train_set, batch_size = params[\"batch_size\"], shuffle = True)\n",
    "dev_set_loader = torch.utils.data.DataLoader(dev_set, shuffle = True)\n",
    "test_set_loader = torch.utils.data.DataLoader(test_set, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, dataset, optimiz, epoch, costF):\n",
    "  model.train()\n",
    "\n",
    "  for batch_idx, (data, target) in enumerate(dataset):\n",
    "    # Move train data to the training device (likely GPU)\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    # Reset gradient\n",
    "    optimiz.zero_grad()\n",
    "\n",
    "    # Forward propagation\n",
    "    out = model(data)\n",
    "\n",
    "    # Loss and backpropagation\n",
    "    cost = costF(out, target)\n",
    "    cost.backward()\n",
    "    optimiz.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, dataset, costF, setName):\n",
    "\t# Set model in evaluation mode\n",
    "\tmodel.eval()\n",
    "\n",
    "\t# Init total loss and correct predictions\n",
    "\tloss = 0.0\n",
    "\tcorrect_pred = 0\n",
    "\n",
    "\tfor id, (data, target) in enumerate(dataset):\n",
    "\t\t# Move train data to the training device (likely GPU)\n",
    "\t\tdata = data.to(device)\n",
    "\t\ttarget = target.to(device)\n",
    "\n",
    "\t\t# Forward propagation\n",
    "\t\tout = model(data)\n",
    "\n",
    "\t\t# Define the model prediction as the class with higher probability\n",
    "\t\tpred = out.argmax(dim=1, keepdim=True)\n",
    "\n",
    "\t\t# Try to reshape the arrays into what they already are\n",
    "\t\t# If there were errors in the setup, this will create errors\n",
    "\n",
    "\t\t# sanity check\n",
    "\t\tbatch_size = data.shape[0]\n",
    "\t\tpred = pred.view(batch_size)  # [bs,]\n",
    "\t\ttarget = target.view(batch_size)  # [bs,]\n",
    "\n",
    "\t\t# Sum the loss of all inputs\n",
    "\t\tloss += costF(out, target, reduction='sum').item()\n",
    "\n",
    "\t\t# Sum the number of correct predictions\n",
    "\t\tcorrect_pred += pred.eq(target).sum().item()\n",
    "\t\t\n",
    "\t\t# print(f\"Item #{id} of {len(dataset)}\")\n",
    "\n",
    "\t# Compute statistics\n",
    "\tnum_samples = len(dataset.dataset)\n",
    "\tavg_loss = loss / num_samples\n",
    "\taccuracy = float(correct_pred) / num_samples\n",
    "\n",
    "\t# Print statistics\n",
    "\t#   print(f\"{setName} ==> Avg epoch loss: {avg_loss:2.04} - accuracy: {accuracy:2.04}\")\n",
    "\n",
    "\treturn avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(type, lr):\n",
    "\tif type == \"Adam\":\n",
    "\t\treturn torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\t\n",
    "\t# if type == \"...\"\n",
    "\t#\treturn torch.optim. ...\n",
    "\t\n",
    "\tparams[\"optimizer\"] = \"SDG\"\n",
    "\treturn torch.optim.SGD(model.parameters(), lr = lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"loss\"] == \"binary cross entropy with logits\":\n",
    "\tloss = torch.nn.functional.binary_cross_entropy_with_logits\n",
    "# elif params[\"loss\"] == \"...\"\n",
    "#\tloss = torch.nn.functional. ...\n",
    "else:\n",
    "\tloss = torch.nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_params_condition(train):\n",
    "\tfor name, param in model.named_parameters():\n",
    "\t\tif not train(name):\n",
    "\t\t\tparam.requires_grad = False \n",
    "\t\telse:\n",
    "\t\t\tparam.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_params_method_0(choice):\n",
    "\tif choice == \"train all\":\n",
    "\t\tfreeze_params_condition(lambda name: True)\n",
    "\n",
    "\tif choice == \"train last\":\n",
    "\t\tfreeze_params_condition(lambda name: name.startswith(\"fc.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_params_method_1(choice):\n",
    "\tif choice == \"train all\":\n",
    "\t\tfreeze_params_condition(lambda name: True)\n",
    "\n",
    "\tif choice == \"train last two\":\n",
    "\t\tfreeze_params_condition(lambda name: name.startswith(\"fc.\"))\n",
    "\n",
    "\tif choice == \"train last\":\n",
    "\t\tfreeze_params_condition(lambda name: name.startswith(\"fc.2.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_params_method_2(choice):\n",
    "\tif choice == \"train all\":\n",
    "\t\tfreeze_params_condition(lambda name: True)\n",
    "\n",
    "\tif choice == \"train last\":\n",
    "\t\tfreeze_params_condition(lambda name: name.startswith(\"1.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_params = [\n",
    "\tfreeze_params_method_0,\n",
    "\tfreeze_params_method_1,\n",
    "\tfreeze_params_method_2,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "dev_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_phase(phase_num, optimiz, epochs):\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttrain(model, device, train_set_loader, optimiz, epoch, loss)\n",
    "\n",
    "\t\ttrain_loss, train_acc = test(model, device, train_set_loader, loss, \"TRAIN\")\n",
    "\t\tdev_loss, dev_acc = test(model, device, dev_set_loader, loss, \"DEV\")\n",
    "\n",
    "\t\ttrain_losses.append(train_loss)\n",
    "\t\tdev_losses.append(dev_loss)\n",
    "\t\tepochs_list = [i for i, _ in enumerate(train_losses)]\n",
    "\n",
    "\t\tprint(f\"| {phase_num:5} | {epoch:5} |\", end=\"\")\n",
    "\t\tprint(f\"  {train_loss:6.3f}  | {100*train_acc:6.2f} % |\", end=\"\")\n",
    "\t\tprint(f\"  {dev_loss:6.3f}  | {100*dev_acc:6.2f} % |\")\n",
    "\n",
    "\t\tplt.figure()\n",
    "\t\tplt.plot(epochs_list, train_losses, label=\"Train\", marker='o')\n",
    "\t\tplt.plot(epochs_list, dev_losses, label=\"Dev\", marker='o')\n",
    "\n",
    "\t\tplt.title(\"Average loss at epochs\")\n",
    "\n",
    "\t\tplt.xlabel(\"Epoch\")\n",
    "\t\tplt.xticks(epochs_list)\n",
    "\n",
    "\t\tplt.ylabel(\"Loss\")\n",
    "\t\tplt.ylim(bottom=0)\n",
    "\n",
    "\t\tplt.legend()\n",
    "\t\tplt.savefig(\"loss.png\")\n",
    "\t\tplt.close()\n",
    "\t\tprint(\"+-------+-------+----------+----------+----------+----------+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "MODEL IN PHASE 1\n",
      "==================================================\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 122, 122]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 122, 122]             128\n",
      "              ReLU-3         [-1, 64, 122, 122]               0\n",
      "         MaxPool2d-4           [-1, 64, 61, 61]               0\n",
      "            Conv2d-5           [-1, 64, 61, 61]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 61, 61]             128\n",
      "              ReLU-7           [-1, 64, 61, 61]               0\n",
      "            Conv2d-8           [-1, 64, 61, 61]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 61, 61]             128\n",
      "             ReLU-10           [-1, 64, 61, 61]               0\n",
      "       BasicBlock-11           [-1, 64, 61, 61]               0\n",
      "           Conv2d-12           [-1, 64, 61, 61]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 61, 61]             128\n",
      "             ReLU-14           [-1, 64, 61, 61]               0\n",
      "           Conv2d-15           [-1, 64, 61, 61]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 61, 61]             128\n",
      "             ReLU-17           [-1, 64, 61, 61]               0\n",
      "       BasicBlock-18           [-1, 64, 61, 61]               0\n",
      "           Conv2d-19          [-1, 128, 31, 31]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 31, 31]             256\n",
      "             ReLU-21          [-1, 128, 31, 31]               0\n",
      "           Conv2d-22          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 31, 31]             256\n",
      "           Conv2d-24          [-1, 128, 31, 31]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 31, 31]             256\n",
      "             ReLU-26          [-1, 128, 31, 31]               0\n",
      "       BasicBlock-27          [-1, 128, 31, 31]               0\n",
      "           Conv2d-28          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 31, 31]             256\n",
      "             ReLU-30          [-1, 128, 31, 31]               0\n",
      "           Conv2d-31          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 31, 31]             256\n",
      "             ReLU-33          [-1, 128, 31, 31]               0\n",
      "       BasicBlock-34          [-1, 128, 31, 31]               0\n",
      "           Conv2d-35          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 16, 16]             512\n",
      "             ReLU-37          [-1, 256, 16, 16]               0\n",
      "           Conv2d-38          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 16, 16]             512\n",
      "           Conv2d-40          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 16, 16]             512\n",
      "             ReLU-42          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-43          [-1, 256, 16, 16]               0\n",
      "           Conv2d-44          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 16, 16]             512\n",
      "             ReLU-46          [-1, 256, 16, 16]               0\n",
      "           Conv2d-47          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 16, 16]             512\n",
      "             ReLU-49          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-50          [-1, 256, 16, 16]               0\n",
      "           Conv2d-51            [-1, 512, 8, 8]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-53            [-1, 512, 8, 8]               0\n",
      "           Conv2d-54            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-56            [-1, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-58            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-59            [-1, 512, 8, 8]               0\n",
      "           Conv2d-60            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-62            [-1, 512, 8, 8]               0\n",
      "           Conv2d-63            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-65            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-66            [-1, 512, 8, 8]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "           ResNet-69                 [-1, 1000]               0\n",
      "           Linear-70                    [-1, 2]           2,002\n",
      "================================================================\n",
      "Total params: 11,691,514\n",
      "Trainable params: 2,002\n",
      "Non-trainable params: 11,689,512\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.68\n",
      "Forward/backward pass size (MB): 76.09\n",
      "Params size (MB): 44.60\n",
      "Estimated Total Size (MB): 121.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for phase in range(params[\"num_phases\"]):\n",
    "\tprint(\"\\n\\n==================================================\")\n",
    "\tprint(f\"MODEL IN PHASE {phase + 1}\")\n",
    "\tprint(\"==================================================\")\n",
    "\tfreeze_params[params[\"method\"]](params[\"train\"][phase])\n",
    "\ttorchsummary.summary(model, (3, 244, 244))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PARAMETERS\n",
      "==================================================\n",
      "\n",
      "Available_methods:\n",
      "  0: replace the FC layer with a new FC\n",
      "  1: replace the FC layer with two new FCs\n",
      "  2: add a FC layer after the last FC, and only train that\n",
      "\n",
      "method: 2\n",
      "\n",
      "batch_size: 32\n",
      "\n",
      "num_phases: 1\n",
      "\n",
      "learning_rates:\n",
      "- 0.001\n",
      "\n",
      "epochs:\n",
      "- 20\n",
      "\n",
      "optimizer: SDG\n",
      "\n",
      "loss: cross entropy\n",
      "\n",
      "train:\n",
      "- train last\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "TRAINING\n",
      "==================================================\n",
      "\n",
      "+-------+-------+---------------------+---------------------+\n",
      "|       |       |      TRAIN set      |       DEV set       |\n",
      "+ Phase + Epoch +----------+----------+----------+----------+\n",
      "|       |       | Avg loss | Accuracy | Avg loss | Accuracy |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |     0 |   0.065  |  97.36 % |   0.060  |  97.88 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |     1 |   0.055  |  97.96 % |   0.054  |  97.88 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |     2 |   0.056  |  97.82 % |   0.069  |  97.62 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |     3 |   0.070  |  97.29 % |   0.084  |  96.62 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |     4 |   0.054  |  97.93 % |   0.060  |  98.00 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |     5 |   0.052  |  97.89 % |   0.060  |  97.88 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |     6 |   0.051  |  98.06 % |   0.056  |  97.75 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |     7 |   0.052  |  98.04 % |   0.066  |  97.38 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |     8 |   0.046  |  98.24 % |   0.059  |  98.00 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |     9 |   0.045  |  98.18 % |   0.067  |  97.75 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |    10 |   0.058  |  97.88 % |   0.061  |  97.75 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |    11 |   0.045  |  98.33 % |   0.054  |  97.25 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |    12 |   0.049  |  98.14 % |   0.079  |  97.12 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |    13 |   0.043  |  98.39 % |   0.065  |  97.88 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |    14 |   0.044  |  98.32 % |   0.062  |  97.88 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |    15 |   0.043  |  98.39 % |   0.054  |  98.12 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |    16 |   0.047  |  98.25 % |   0.073  |  97.50 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |    17 |   0.045  |  98.31 % |   0.067  |  97.75 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |    18 |   0.041  |  98.40 % |   0.063  |  98.12 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n",
      "|     0 |    19 |   0.048  |  98.02 % |   0.074  |  97.75 % |\n",
      "+-------+-------+----------+----------+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "print(\"==================================================\")\n",
    "print(\"PARAMETERS\")\n",
    "print(\"==================================================\\n\")\n",
    "for key, val in params.items():\n",
    "\tprint(yaml.dump({key: val}))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"TRAINING\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "print(\"+-------+-------+---------------------+---------------------+\")\n",
    "print(\"|       |       |      TRAIN set      |       DEV set       |\")\n",
    "print(\"+ Phase + Epoch +----------+----------+----------+----------+\")\n",
    "print(\"|       |       | Avg loss | Accuracy | Avg loss | Accuracy |\")\n",
    "print(\"+-------+-------+----------+----------+----------+----------+\")\n",
    "\n",
    "for phase in range(params[\"num_phases\"]):\n",
    "\toptimiz = get_optimizer(params[\"optimizer\"], params[\"learning_rates\"][phase])\n",
    "\tepochs = params[\"epochs\"][phase]\n",
    "\tfreeze_params[params[\"method\"]](params[\"train\"][phase])\n",
    "\ttraining_phase(phase, optimiz, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
